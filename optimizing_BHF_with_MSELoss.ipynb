{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMg8EvRANcgFoJOnvotYk2z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingyeom121212/aipython/blob/main/optimizing_BHF_with_MSELoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('DATA2.csv')\n",
        "\n",
        "# 특성과 타겟 분리\n",
        "X = data[['BHF1', 'BHF2', 'BHF3', 'BHF4', 'BHF5']].values\n",
        "y = data[['W2', 'W4', 'Crack']].values\n",
        "\n",
        "# 데이터 표준화\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y)\n",
        "\n",
        "# 학습 및 테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터 텐서로 변환\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# 모델 정의\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.fc3 = nn.Linear(10, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# 손실 함수 및 최적화 방법 정의\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 모델 학습\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    loss = criterion(outputs, y_test)\n",
        "    print(f'Test Loss: {loss.item():.4f}')\n",
        "\n",
        "# 최적의 BHF 값을 찾기 위해 최적화\n",
        "BHF_initial = torch.tensor(scaler_X.transform([[0, 0, 0, 0, 0]]), dtype=torch.float32, requires_grad=True)\n",
        "optimizer = optim.Adam([BHF_initial], lr=0.01)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(BHF_initial)\n",
        "    loss = criterion(outputs, torch.tensor([[0, 0, 0]], dtype=torch.float32))  # 목표 W2, W4, Crack 값\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Optimization Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "optimal_BHF = scaler_X.inverse_transform(BHF_initial.detach().numpy())\n",
        "print(f'Optimal BHF values: {optimal_BHF}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqfYvgX-hNGa",
        "outputId": "0c1f8d64-58a5-402c-e6be-fce98f4bf0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.0667\n",
            "Epoch [200/1000], Loss: 0.0345\n",
            "Epoch [300/1000], Loss: 0.0220\n",
            "Epoch [400/1000], Loss: 0.0171\n",
            "Epoch [500/1000], Loss: 0.0142\n",
            "Epoch [600/1000], Loss: 0.0111\n",
            "Epoch [700/1000], Loss: 0.0069\n",
            "Epoch [800/1000], Loss: 0.0044\n",
            "Epoch [900/1000], Loss: 0.0037\n",
            "Epoch [1000/1000], Loss: 0.0031\n",
            "Test Loss: 0.9060\n",
            "Optimization Epoch [100/1000], Loss: 0.0575\n",
            "Optimization Epoch [200/1000], Loss: 0.0429\n",
            "Optimization Epoch [300/1000], Loss: 0.0303\n",
            "Optimization Epoch [400/1000], Loss: 0.0200\n",
            "Optimization Epoch [500/1000], Loss: 0.0123\n",
            "Optimization Epoch [600/1000], Loss: 0.0071\n",
            "Optimization Epoch [700/1000], Loss: 0.0039\n",
            "Optimization Epoch [800/1000], Loss: 0.0020\n",
            "Optimization Epoch [900/1000], Loss: 0.0009\n",
            "Optimization Epoch [1000/1000], Loss: 0.0004\n",
            "Optimal BHF values: [[ -5390.9795  37027.727  334302.25    38703.37    85877.09  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화 단계에서 음수가 발생하여 최적화 단계에서 BHF값이 음수가 되지 않도록 Clamping 하였음\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('DATA2.csv')\n",
        "\n",
        "# 특성과 타겟 분리\n",
        "X = data[['BHF1', 'BHF2', 'BHF3', 'BHF4', 'BHF5']].values\n",
        "y = data[['W2', 'W4', 'Crack']].values\n",
        "\n",
        "# 데이터 표준화\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y)\n",
        "\n",
        "# 학습 및 테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터 텐서로 변환\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# 모델 정의\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.fc3 = nn.Linear(10, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# 손실 함수 및 최적화 방법 정의\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 모델 학습\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    loss = criterion(outputs, y_test)\n",
        "    print(f'Test Loss: {loss.item():.4f}')\n",
        "\n",
        "# 최적의 BHF 값을 찾기 위해 최적화\n",
        "BHF_initial = torch.tensor(scaler_X.transform([[0, 0, 0, 0, 0]]), dtype=torch.float32, requires_grad=True)\n",
        "optimizer = optim.Adam([BHF_initial], lr=0.01)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(BHF_initial)\n",
        "    loss = criterion(outputs, torch.tensor([[0, 0, 0]], dtype=torch.float32))  # 목표 W2, W4, Crack 값\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # BHF 값을 음수가 되지 않도록 클램핑\n",
        "    with torch.no_grad():\n",
        "        BHF_initial.clamp_(min=0.0)\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Optimization Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "optimal_BHF = scaler_X.inverse_transform(BHF_initial.detach().numpy())\n",
        "print(f'Optimal BHF values: {optimal_BHF}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kInWQCKHmdrM",
        "outputId": "45c5874c-0a73-4ffc-e9c7-c89cf0380e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.0751\n",
            "Epoch [200/1000], Loss: 0.0396\n",
            "Epoch [300/1000], Loss: 0.0214\n",
            "Epoch [400/1000], Loss: 0.0180\n",
            "Epoch [500/1000], Loss: 0.0165\n",
            "Epoch [600/1000], Loss: 0.0157\n",
            "Epoch [700/1000], Loss: 0.0151\n",
            "Epoch [800/1000], Loss: 0.0144\n",
            "Epoch [900/1000], Loss: 0.0133\n",
            "Epoch [1000/1000], Loss: 0.0111\n",
            "Test Loss: 0.3189\n",
            "Optimization Epoch [100/1000], Loss: 0.0001\n",
            "Optimization Epoch [200/1000], Loss: 0.0001\n",
            "Optimization Epoch [300/1000], Loss: 0.0001\n",
            "Optimization Epoch [400/1000], Loss: 0.0001\n",
            "Optimization Epoch [500/1000], Loss: 0.0001\n",
            "Optimization Epoch [600/1000], Loss: 0.0001\n",
            "Optimization Epoch [700/1000], Loss: 0.0001\n",
            "Optimization Epoch [800/1000], Loss: 0.0001\n",
            "Optimization Epoch [900/1000], Loss: 0.0001\n",
            "Optimization Epoch [1000/1000], Loss: 0.0001\n",
            "Optimal BHF values: [[ 24857.37   34418.395 468782.84  119186.42  232086.62 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#위에서는 W2,W4,Crack에 대한 BHF값들을 한번에 최적화 하였다면 아래의 코드는\n",
        "#각각의 W2,W4,Crack에 대한 BHF값들을 최적화 하고 그 값들의 평균을 내었음\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('DATA2.csv')\n",
        "\n",
        "# 특성과 타겟 분리\n",
        "X = data[['BHF1', 'BHF2', 'BHF3', 'BHF4', 'BHF5']].values\n",
        "y = data[['W2', 'W4', 'Crack']].values\n",
        "\n",
        "# 데이터 표준화\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y)\n",
        "\n",
        "# 학습 및 테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터 텐서로 변환\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# 모델 정의\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.fc3 = nn.Linear(10, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# 손실 함수 및 최적화 방법 정의\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 모델 학습\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    loss = criterion(outputs, y_test)\n",
        "    print(f'Test Loss: {loss.item():.4f}')\n",
        "\n",
        "# 최적의 BHF 값을 찾기 위한 함수\n",
        "def find_optimal_BHF(target_index):\n",
        "    BHF_initial = torch.tensor(scaler_X.transform([[0, 0, 0, 0, 0]]), dtype=torch.float32, requires_grad=True)\n",
        "    optimizer = optim.Adam([BHF_initial], lr=0.01)\n",
        "    target_tensor = torch.zeros((1, 3), dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(BHF_initial)\n",
        "        loss = criterion(outputs[0, target_index], target_tensor[0, target_index])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # BHF 값을 음수가 되지 않도록 클램핑\n",
        "        with torch.no_grad():\n",
        "            BHF_initial.clamp_(min=0.0)\n",
        "\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f'Optimization for target {target_index} Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return BHF_initial.detach().numpy()\n",
        "\n",
        "# W2, W4, Crack 각각에 대해 최적화된 BHF 값을 구함\n",
        "optimal_BHF_W2 = find_optimal_BHF(0)\n",
        "optimal_BHF_W4 = find_optimal_BHF(1)\n",
        "optimal_BHF_Crack = find_optimal_BHF(2)\n",
        "\n",
        "# 최적화된 BHF 값의 평균 계산\n",
        "optimal_BHF_avg = (optimal_BHF_W2 + optimal_BHF_W4 + optimal_BHF_Crack) / 3\n",
        "optimal_BHF_avg = scaler_X.inverse_transform(optimal_BHF_avg)\n",
        "print(f'Optimal BHF values (average): {optimal_BHF_avg}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH64HBXTn6sv",
        "outputId": "1f6a02de-66ce-43ac-fd8b-bc21d8613be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.0733\n",
            "Epoch [200/1000], Loss: 0.0334\n",
            "Epoch [300/1000], Loss: 0.0206\n",
            "Epoch [400/1000], Loss: 0.0148\n",
            "Epoch [500/1000], Loss: 0.0082\n",
            "Epoch [600/1000], Loss: 0.0051\n",
            "Epoch [700/1000], Loss: 0.0038\n",
            "Epoch [800/1000], Loss: 0.0035\n",
            "Epoch [900/1000], Loss: 0.0032\n",
            "Epoch [1000/1000], Loss: 0.0031\n",
            "Test Loss: 11.9652\n",
            "Optimization for target 0 Epoch [100/1000], Loss: 0.0008\n",
            "Optimization for target 0 Epoch [200/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [300/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [400/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [500/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [600/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [700/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [800/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [900/1000], Loss: 0.0000\n",
            "Optimization for target 0 Epoch [1000/1000], Loss: 0.0000\n",
            "Optimization for target 1 Epoch [100/1000], Loss: 0.0314\n",
            "Optimization for target 1 Epoch [200/1000], Loss: 0.0269\n",
            "Optimization for target 1 Epoch [300/1000], Loss: 0.0191\n",
            "Optimization for target 1 Epoch [400/1000], Loss: 0.0122\n",
            "Optimization for target 1 Epoch [500/1000], Loss: 0.0121\n",
            "Optimization for target 1 Epoch [600/1000], Loss: 0.0120\n",
            "Optimization for target 1 Epoch [700/1000], Loss: 0.0120\n",
            "Optimization for target 1 Epoch [800/1000], Loss: 0.0118\n",
            "Optimization for target 1 Epoch [900/1000], Loss: 0.0116\n",
            "Optimization for target 1 Epoch [1000/1000], Loss: 0.0115\n",
            "Optimization for target 2 Epoch [100/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [200/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [300/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [400/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [500/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [600/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [700/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [800/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [900/1000], Loss: 0.0000\n",
            "Optimization for target 2 Epoch [1000/1000], Loss: 0.0000\n",
            "Optimal BHF values (average): [[ 31637.336  33472.76  462289.06  125514.875 238150.39 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IyNeRRxPpMaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}